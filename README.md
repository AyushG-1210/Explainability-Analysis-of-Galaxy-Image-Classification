# Abstract
Deep learning models have been advanced to excel at Galaxy image classification, yet there's a limit to their trust and utility. This is due to their inherent "black box" nature, which hides the thinking and reasoning of the deep learning framework. A quantitative comparison of popular Explainable AI (XAI) methods is needed to determine their practical trade-offs in an astronomical context. This can be achieved by evaluating three prominent XAI techniques i.e. Grad-CAM, LIME, and DeepSHAP, by applying to a fine-tuned ResNet-18 model for four-class galaxy classification. I quantitatively assessed each method based on computational efficiency and explanation fidelity. The obtained results reveal that there lies a trade-off: Grad-CAM, despite being the least computationally efficient in my implementation, demonstrated significantly higher fidelity, proving its explanations were most aligned with the model's core reasoning. This work concludes that for scientific applications where trustworthiness is paramount, explanation fidelity outweighs computational cost, making Grad-CAM the recommended tool. These findings underscore the importance of careful method selection for generating reliable scientific insights and suggest future work should validate these trade-offs on larger-scale models.
> Keywords: Explainable AI (XAI), Interpretability, Galaxy Classification, Deep Learning, LIME, Grad-CAM, DeepSHAP, Fidelity, Astroinformatics
